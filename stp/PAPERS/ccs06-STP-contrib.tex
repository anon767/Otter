\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{floatflt}
\usepackage{amsmath}
\usepackage{url}
\usepackage{subfigure}

\newcommand{\stpread}{{\it read}}
\newcommand{\stpwrite}{{\it write}}
\newcommand{\stpite}{{\it ite}}

\date{}
\title{System Description of STP}

\author{
Vijay Ganesh and David L. Dill \\
\\
Computer Systems Laboratory \\
Stanford University, Stanford, CA, USA \\
\{vganesh, dill\} @cs.stanford.edu}

\begin{document}

\maketitle

This section describes a core contributions of this paper, STP (Simple
Theorem Prover), a custom constraint solver we designed and built
explicitly to handle the types of constraints encountered when using
EXE, and how EXE uses its primitives to build an accurate view of
memory.

Traditionally, constraint solvers have been built with the assumption
that SAT is slow and weak.  For decades, their main trick to avoid it
has been to use one of many variations on a venerable technique,
Nelson and Oppen's cooperating decision procedures
framework~\cite{NelsonOppen}.  Minimizing reliance on SAT comes at
significant complexity and cost --- each theory must communicate with
the others whenever it discovers an equality and these communications
can add significant overhead and, if one is forgotten, subtle errors
in the solver.  We have first hand experience with both of these
implications in our work with CVCL~\cite{cvcl}, the previous
constraint solver used by EXE.  The complexity of Nelson-Oppen led to
complex dependencies, which made the code difficult to understand,
tune, or get right.

However, while this approach has been the dominant ideology of constraint
solvers, in the meantime SAT solvers have changed dramatically to become
voracious and fast.  Increasingly the sensible thing is to embrace
rather than avoid them, since the operations one attempts to emulate at
a higher-level will likely be slower than what SAT provides.

This is why we decided to write our custom constraint solver, STP.
Instead of a Nelson-Oppen combination, STP eagerly translates the
problem into a purely propositional problem for a SAT algorithm.  No
proofs are generated, there are no separate theories, and no
equalities have to be communicated.  STP, like CVCL before it,
basically takes as input a sequence of assertions.  These are all
logically ANDed into a single formula, which STP checks for
satisfiability.  Specifically, a formula such as $x < y$ is
interpreted as n-bit arithmetic, and is translated into the Boolean
gate-level circuit for a signed or unsigned bit-vector comparator,
which is then translated into CNF for the SAT solver.  To solve SAT
queries, we use MiniSAT~\cite{Minisat}, an out-of-the-box SAT solver.
Currently the code size of STP is roughly five times smaller than CVCL
but, even more importantly, has a more ``horizontal'' complexity since
the pieces can work in isolation.  Modularity and simplicity help
constraint solvers as they do everything else.  In a sense, STP can be
viewed as the result of applying a systems approach to constraint
solving that has worked so well in the context of SAT: start simple,
measure bottlenecks on real workloads, and tune to exactly
these cases.

We first give a high-level overview of what EXE requires for modeling
memory, and how STP meets these requirements.  Then, we present the
optimizations STP performs, and show experimental numbers of why these
optimizations matter.

\subsection{EXE's view of memory}

Because of space limits the reader gets only a cursory treatment EXE's
implementation; for a more thorough discussion see~\cite{exe:disks}.
When executing a program, EXE keeps track of two memory stores: the
concrete store, which includes the heap, the stack and the data
segments, and the symbolic store, which includes the symbolic
variables used in the current set of constraints, and the set of
symbolic constraints themselves. Each bit in the symbolic store has a
unique corresponding bit in the concrete store.  However, concrete
bits holding concrete values have no corresponding storage in the
symbolic store.

All values start out as concrete.  When the user marks a set of bytes
as symbolic, EXE creates a corresponding, identically-sized range of
bytes in the symbolic store, and records this correspondence in a hash
table that maps byte addresses to their corresponding symbolic bytes.
As the program executes, this table grows as more bytes become
symbolic, either by assigning a symbolic expression to a concrete
variable (parameter passing can be viewed as a form of assignment) or
by indexing a data block by a symbolic index.

Accurately tracking constraints that only involve strongly-typed
scalar variables is relatively simple: just name the variables uniquely
(e.g., as in the original code) and use these names consistently in the
constraints.  There are two problems we had to handle for real C code.
First, system code often treats memory as untyped bytes, and observes
a single memory location in multiple ways.
For example, by casting signed
variables to unsigned, or (in the code we checked) treating an array
of bytes as a network packet, inode, packet filter, etc. 
Second, prolific use of symbolic pointer expressions.  Unlike scalars or
concrete pointers, symbolic pointers simultaneously refer to many different
symbolic locations.  For example, a simple symbolic assignment of the form
$a[k] = x$ could update any location in $a$ that the symbolic index $k$
could refer to.  Similarly, a read from $a$ at a symbolic index could
return any element in $a$ that the index can refer to. 
We discuss these problems and STP in more detail below.

\subsection{What STP provides to model memory}
\label{sec:untyped:bytes}

STP provides two datatypes: bitvectors (both symbolic and concrete)
and arrays.  A bitvector is a fixed-length sequence of untyped bits,
used to store numeric values.  For example, the concrete {\tt 0010} is
a bitvector of size four representing the constant 2.  Bitvector
operations include all the standard C arithmetic operations (including
non-linear operations such as multiplication, division and modulo),
bitwise boolean operations, relational operations (less than, less
than or equal, etc.), and multiplexers, which provide an
``if-then-else'' construct that is converted into a logical formula.
In addition, two important bitvector operations are bit concatenation
and bit extraction, which EXE relies on to perform translate untyped
memory into properly-typed constraints.

STP implements its bitvector operations by translating them to operations
on individual bits.  There are two varieties of expressions: {\em
terms}, which have bitvector values, and {\em formulas\/}, which have
Boolean values.  If $x$ and $y$ are 32-bit bitvector values, $x + y$
is a term returning a 32-bit result, and $x + y < z$ (where $<$ is an
unsigned 32-bit comparison) is a formula.  In the implementation, terms
are converted into vectors of Boolean formulas consisting entirely of
single bit operations (AND, XOR, etc.).  Each operation is converted
in a fairly obvious way; for example, a 32-bit add is implemented as
a ripple-carry adder.  Formulas are converted into DAGs of single bit
operations, where
expressions with identical structure are represented uniquely
(expression nodes are looked up in a hash table whenever they are
created, to see whether an identical node already exists).
Simple Boolean optimizations are applied as the nodes are
created; for example, a call to create a node for {\tt AND(x, FALSE)}
will just return the {\tt FALSE} node.  The resulting Boolean DAG is
then converted to CNF by the standard method of naming intermediate
nodes with new propositional variables.  With the exception of merging
2-argument ANDs and ORs into n-ary ANDs and ORs, this translation has
remained completely unoptimized, because, to date, it has not emerged
as a serious performance bottleneck.


\subsection{The key to speed: fast array constraints}
\label{sec:stp:arrays}

In EXE, the efficiency of reasoning about {\em arrays\/} is almost
always the primary performance bottleneck.  Unless there is a
fairly large array or a difficult operation like multiplication of
two non-constants, the constraints are almost always solved in a
fraction of a second.  Therefore, we discuss handling of arrays 
in more detail below.


STP is an implementation of logic, so it is a purely functional
language.  The logic has one-dimensional arrays that can be
indexed by bitvectors and can contain bitvectors.  The operations on
arrays are $\stpread(A, i)$, which returns the value at location
$A[i]$ when $A$ is an array and $i$ is an index expression of the
correct type, and $\stpwrite(A, i, v)$, which returns a new array
which has the same value as $A$ at all indices except possibly $i$, in
which case the new array has the value $v$.  

The only other operation on arrays is that they can appear in as the
{\tt then} and {\tt else} subexpressions of an {\tt if-then-else}
expression (an if-then-else expression is written ${\it ite}(x, y,
z)$, where $x$ is the condition, $y$ the then expression, and $z$ the
else expression).

Before the bitvector translation step, array expressions are
eliminated from the input expression by a transformation phase.
There are several transformations.  The following two eliminate
all \stpwrite\ nodes:

\begin{tabular}{ll}

{\bf if-lifting} &  $\stpread(\stpite(x, y, z), i) 
	\Rightarrow \stpite(x, \stpread(y, i), \stpread(z, i))$  \\


{\bf read-over-write} &   $\stpread(\stpwrite(A, i, v), j) 
	\Rightarrow \stpite(i=j, v, \stpread(A, j))$ \\

\end{tabular}

if-lifting moves the {\stpread}s down until they are immediately above
{\stpwrite}s, where read-over-write can eliminate them.

{\stpread}s are then eliminated via a transformation mentioned in
~\cite{Uclid}.  All of the terms that are ever used as indices to
{\stpread}s for each array in the input, are collected and assigned
global total order. To illustrate this transformation consider the
formula $\stpread(A,i_0) = e_0 \wedge \stpread(A,i_i) = e_1 \wedge
\stpread(A,i_2)=e_2$. The transformed result would be $v_0 = e_0 \wedge
ite(i_1=i_0,v_0,v_1)=e_1 \wedge
ite(i_2=i_1,ite(i_1=i_0,v_0,v_1),v_2)=e_2$. This transformation
essentially captures the axiom that if two indices are the same then
the array $\stpread$s should return the same value.

Read elimination does not expand expressions as much as it may appear,
because of the DAG representation (the space cost of repeating an
expression is just a pointer).  However, if there are $n$
syntactically distinct index expressions in $\stpread$s, and since
each index has to be compared with all previous indices in the total
order, it results in $O(n^2/2)$ additional nodes.  Unfortunately, this
blowup is lethal for arrays of a few thousand elements, which occur
frequently in EXE.  Fortunately, the problems generated by EXE are
amenable to special optimizations.

The first optimization is to reduce the number of array variables by
substituting out all assertions of the form $\stpread(A, c_1) = e$, where $c_1$
is a constant and $e$ does not contain 
%$\stpread(A, c_1)$.  
another array read.
This is a two-pass
algorithm.  The first pass builds a substitution table with the
left-hand-side of each such equation as the key and the
right-hand-side as the value and then deletes the equation.
For soundness, if an equation is encountered whose left-hand-side
is already in the table, the equation is not deleted and the table
is not changed.  The second pass over the expression replaces each
occurrence of a key by the corresponding table entry.

The second optimization is to delay the translation of array
{\stpread}s with non-constant indices, in effect introducing some
laziness into STP's handling of arrays, in the hope of avoiding the
$O(n^2)$ blowup from the $\stpread$-elimination transformation. We
call this optimization SAT-based refinement. 

Initially, all array read expressions are replaced by simple variables
to yield an approximation of the original formula.  The resulting
logical formula is under-constrained, and correct, except that it is
ignoring the important {\em substitution axiom:} $\forall x\ \forall
y\ (x = y \Rightarrow
\stpread(A,x) =
\stpread(A,y))$, which requires that array reads return the same
values when the indices are the same. If the resulting
under-constrained formula is not satisfiable, there is indeed no
solution for the original formula and STP returns unsatisfiable, and
the result is correct. 

If, however, the SAT solver finds a solution to the under-constrained
formula, then that solution is not guaranteed to be correct because it
could violate the substitution axiom.  For example, it could be given
the assertion $A[0] = 0 \wedge A[i] = 1$, which would be translated to
$v_1 = 1$, and STP might find the ``solution'' $i = 0$ and $v_1 =
1$. 

STP then maps the solution over the variables of the under-constrained
formula to the variables of the original formula. Under this mapped
solution, the original formula evaluates to $A[0] = 1$, and further to
$0=1$, which is clearly false. Hence, the solution to the
underconstrained formula is not a solution to the original formula.
If the original formula is true under the mapped solution, STP
terminates, since it has found a true satisfying assignment.

More formally, if the supposed satisfying assignment returned on the
first try {\em fails\/} to make the original formula true, it must be
because some substitution axiom was violated.  In this case, the
algorithm systematically goes through all substitution axioms and
checks whether it is true given the current satisfying assignment.
When it finds a false instance of the axiom (which it must), it
logically {\em AND}'s that instance with the underconstrained formula
and repeats the process of finding a satisfying assignment by calling
the SAT solver.  The result must satisfy the newly added axiom
instance, which the previous assignment violated, so this algorithm
will not repeat assignments and will not violate previously added
axioms. This process must terminate since there are only finitely many
substitution axioms.

Consider the example $A[0] = 0 \wedge A[i] = 1$ mentioned above, where
$i$ is a single bit variable. In the first try STP translates this to
$v_1 = 1$, and STP might find the ``solution'' $i = 0$ and $v_1 =
1$. Clearly, the original formula is false in the mapped version of
this solution. So, the algorithm adds the missing axiom $\forall i
(i=0) \implies (A[i]=0)$, and calls the SAT solver again. This time,
the solver finds the correct solution $i=1$ and $v_1=1$. 

In the worst case, the algorithm will add all $(n-1)n/2$ axiom
instances, at which time it is guaranteed to return a correct result
because there are no more axioms it can violate.  However, in many
cases, this loop will terminate quickly because the formula can be
proved unsatisfiable without all the axiom instances, or because it
luckily found a true satisfying assignment without adding all the
axiom instances.  In effect, the algorithm optimistically assumes that
the axiom instances are not needed, and only adds them when this
assumption is shown to be wrong.

These optimizations have made it possible to deal with fairly large
constant arrays when there are relatively few non-constant index expressions,
which is sufficient to permit considerable progress in using EXE on
real examples.  

\end{document}
