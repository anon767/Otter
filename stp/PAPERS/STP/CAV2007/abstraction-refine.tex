\section{Arrays}
As was mentioned above, arrays are used heavily in software analysis
applications, and reasoning about arrays has been a major bottleneck
in many examples. STP's input language supports one-dimensional
(non-extensional) arrays~\cite{stumparray} that are indexed by
bit-vectors and contain bit-vectors.  The operations on arrays are
$\stpread(A, i)$, which returns the value at location $A[i]$ where $A$
is an array and $i$ is an index expression of the correct type, and
$\stpwrite(A, i, v)$, which returns a new array with the same value as
$A$ at all indices except possibly $i$, where it has the value $v$.
The value of a $\stpread$ is a bit-vector, which can appear as an
operand to any operation or predicate that operates on bit-vectors.
The value of an array variable or an array write has an array type,
and may only appear as the first operand of a $\stpread$ or
$\stpwrite$, or as the then or else operand of an if-then-else.  In
particular, values of an array type cannot appear in an equality or
any other predicate.


%% Organization: Show the simple, complete way to handle it, then talk
%% about doing the simple optimizations first, then talk about
%% refinement.

%% Where do these operations all fit into Figure 1?
%% \comment{Need to explain the syntax, ite's, etc.}

In the unoptimized mode, STP reduces all formulas to an
equisatisfiable form that contains no array $\stpread$s or
$\stpwrite$s, using three transformations.  (In the following, the
expression $\stpite(c_1,e_1,e_2)$ is shorthand for {\it if $c_1$ then
$e_1$ else $e_2$ endif})

The first transformation, called {\bf ite-lifting}, transforms
$\stpread(\stpite(cond, \stpwrite(A, i, v),$ $elsepart), j)$ to
$\stpite(cond, \stpread(\stpwrite(A, i, v), j), elsepart)$.  (There is
a similar transformation when the $\stpwrite$ is in the ``else'' part
of the $\stpite$.)  The {\bf read-over-write} transformation
eliminates all write terms by transforming $\stpread(\stpwrite(A, i,
v), j)$ to $\stpite(i=j, v, \stpread(A, j))$.  Finally, the {\bf read
elimination} transformation eliminates $\stpread$ terms by introducing
a fresh bit-vector variable for each such expression, and adding
additional predicates to ensure consistency.  Specifically, whenever a
term $\stpread(A, i)$ appears, it is replaced by a fresh variable $v$,
and new predicates are conjoined to the formula $i = j \Rightarrow v =
w$ for all variables $w$ introduced in place of read terms
$\stpread(A, j)$, having the same array term as first operand. As an
example of this transformation, the simple formula $(\stpread(A, 0) =
0) \wedge (\stpread(A, i) = 1)$ would be transformed to

\[v_1 = 0 \wedge v_2 = 1 \wedge (i = 0 \Rightarrow v_1 = v_2) \]

We refer to the formula of the form $(i = 0 \Rightarrow v_1 = v_2)$ as
an {\it array read axiom}. UCLID also employs read
elimination~\cite{uclid}, but uses a different way to ensure
consistency, which is not amenable to abstraction-refinement. In
particular, the above formula would be transformed into

\[v_1 = 0 \wedge \stpite(i=0,v_1,v_2) = 1 \]

%% \noindent
%% For example, the formula:
%% \[ (\stpread(A, i_1) = e_1) \wedge (\stpread(A, i_2) = e_2) \wedge 
%%   (\stpread(A, i_3) = e_3) \]

%% \noindent would be transformed to

%% \[ (v_1 = e_1) \wedge (v_2 = e_2) \wedge (v_3 = e_3) \wedge 
%%   (i_1 = i_2 \Rightarrow v_1 = v_2) \wedge \\

%%   (i_1 = i_3 \Rightarrow v_1 = v_3) \wedge (i_2 = i_3 \Rightarrow v_2 = v_3) \]

\subsection{Optimizing array reads}

Read elimination, as described above, expands each formula by up to
$n(n-1)/2$ nodes, where $n$ is the number of syntactically distinct
index expressions. Unfortunately, software analysis applications can
produce thousands of reads with variable indices, resulting in a
lethal blow-up when this transformation is applied.  While we are not
able to avoid this blowup in contrived worst-case examples,
appropriate procrastination leads to practical solutions for many very
large problems. We mention two optimizations which have been very
effective: {\it array substitution} and abstraction-refinement for
reads which we call {\it read refinement}.

%% \comment{The novelty of the substition is in the two heuristic
%%   restrictions: constant index, no reads in RHS.  We need to explain
%%   why these are necessary or helpful in practice.}

The array substitution optimization reduces the number of array
variables by substituting out all constraints of the form $\stpread(A,
c) = e_1$, where $c$ is a constant and $e_1$ does not contain another
array read.  Programs often index into arrays or memory using constant
indexes, so this is a case that occurs often in practice.

%% \comment{ (1) why is this two passes?  (2) what if we have
%% $\stpread(A, 1) = \stpread(A, 2) + 1$ \\
%% and later \\
%% $\stpread(A, 2) = x$.
%% }

The optimization has two passes. The first pass builds a substitution
table with the left-hand-side of each such equation ($\stpread(A, c)$)
as the key and the right-hand-side ($e_1$) as the value, and then
deletes the equation from the input query. The second pass over the
expression replaces each occurrence of a key by the corresponding
table entry. Note that for soundness, if we encounter a second
equation whose left-hand-side is already in the table, the second
equation is not deleted and the table is not changed. For our example,
if we saw a subsequent equation $\stpread(A, c) = e_2$ we would leave
it; the second pass of the algorithm would rewrite it as $e_1 = e_2$.

The second optimization, {\it read refinement}, delays the translation
of array {\stpread}s with non-constant indexes in the hope of avoiding
read elimination blowup. Its main trick is to solve a less-expensive
approximation of the formula, check the result in the original
formula, and try again with a more accurate approximation if the
result is incorrect.

Read formulas are abstracted by performing read elimination,
{\em i.e.,} replace reads with new variables, but not adding the array read
axioms.  This abstracted formula is processed by the remaining
stages of STP.  As discussed in the overview, if the result is
unsatisfiable, that result is correct and can be returned immediately
from STP.  If not, the abstract model found by STP is converted to a
concrete model and the original formula is evaluated with respect to
that model.  If the result is $\stptrue$, the answer is correct and
STP returns that model.  Otherwise, some of the array read axioms from
read elimination are added to the formula and STP is asked to satisfy
it again.  This iteration repeates until a correct result is found,
which is guaranteed to happen (if memory and time are not exhausted)
because all of the finitely many array read axioms will eventually be
added in the worst case.

The choice of which array read axioms to add during refinement is a
heuristic that is important to the success of the method.  A policy
that seems to work well is to find a non-constant array index term for
which at least one axiom is violated, then add all of the violated
axioms involving that term.  Adding at least one false axiom during
refinement guarantees that STP will not find the same false model more
than once.  Adding all the axioms for a particular term seems
empirically to be a good compromise between adding just one formula,
which results in too many iterations, and adding all formulas, which
eliminates all abstraction after the first failure.

%% \comment{Use ``substitution formula'' consistently}

For example, suppose STP is given the formula $(\stpread(A, 0) = 0)
\wedge (\stpread(A, i) = 1)$.  STP would first apply the substitution
optimization by deleting $\stpread(A, 0) = 0$ from the formula, and
inserting the pair $(\stpread(A, 0), 0)$ in the substitution
table. Then, it would replace $\stpread(A, i)$ by a new variable
$v_i$, thus generating the under-constrained formula $v_i = 1$.
Suppose STP finds the solution $i = 1$ and $v_i = 1$~\footnote{As
implemented, if $i$ is unconstrainted then STP will always assign it
to be $0$. STP can be programmed to assign random values to
unconstrainted variables, and hence this example is valid}.  STP then
translates the solution to the variables of the original formula to
get $(\stpread(A, 0) = 0)$ $\wedge$ $\stpread(A, 1) = 1)$.  This
solution is satisfiable in the original formula as well, so STP
terminates since it has found a true satisfying assignment.

However, suppose that STP finds the solution $i = 0$ and $v_i = 1$.
Under this solution, the original formula eventually evaluates to
$\stpread(A,0) = 0 \wedge \stpread(A, 0) = 1$, which after
substitution gives $0=1$. Hence, the solution to the under-constrained
formula is not a solution to the original formula.

In this case, STP adds the array read axiom $i=0 \Rightarrow
\stpread(A, i) = \stpread(A, 0)$.  When this formula is checked, the
result must be correct because there are no more axioms to be added.

\subsection{Optimizing array writes}

Eficiently dealing with array writes is crucial to STP's utility in
software applications, some of which produce deeply nested write terms
when there are many successive assignments to indices of the same array.
The {\bf read-over-write} transformation
creates a performance bottleneck by destroying sharing of subterms,
creating an unacceptable blow-up in DAG size.
To see this, consider the simple formula:
$\stpread(\stpwrite(A,i,v),j) = \stpread(\stpwrite(A,i,v),k)$, in
which the $\stpwrite$ term is shared.

The {\bf read-over-write} transformation translates this to
$\stpite(i=j,v,\stpread(A,j)) = \stpite(i=k,v,\stpread(A,k))$.  When
applied recursively to the deeply nested $\stpwrite$ terms, it
essentially creates a new copy of the entire DAG of write terms for
every distinct read index, which exhausts memory in large examples.

%% To see this, consider the scenario where a piece of code is being
%% symbolically simulated, and whose memory is represented as a giant
%% array. Every write to a symbolic (non constant) index in memory means
%% that writes are layered on top of each other (you don't know where you
%% are writing in memory, and hence have to consider the possibility of
%% overwrite). Also, every symbolic read from memory means that the
%% layered writes are now shared across all the symbolic reads. The
%% application of the above transformation results in a quadratic blow-up
%% of the thousands level deep read-over-write, i.e. millions of new
%% equality nodes being created as if-then-else conditionals. This is
%% often a lethal blow to any solver.

Once again, the {\it procrastination principle} applies.  The
{\bf read-over-write} transformation is delayed until after other
simplification and solving transformations are performed.  In
practice, these transformations converted many index terms to
constants.  When {\bf read-over-write} is finally applied, it is
more often applied to terms with constant read and write indices,
causing the conditional to be transformed immediately to 
$\stptrue$ or $\stpfalse$, and collapsed to the term in the ``then''
or ``else'' position as appropriate.
This simple optimization is enormously effective, enabling STP to
solve many very large problems with nested writes that it is otherwise
unable to do.

Abstraction and refinement can also be used on write expressions, when
the previous optimization leaves large numbers of $\stpread$s and
$\stpwrite$s. For this optimization, array read-over-write terms are
replaced by new variables to yield a conjunction of formulas that is
equisatisfiable to the original set.  The example above is transformed
to:
\begin{eqnarray}
v_1 = v_2 \nonumber\\
v_1 = \stpite(i=j,v,\stpread(A,j)) \nonumber\\
v_2 = \stpite(i=k,v,\stpread(A,k)) \nonumber
\end{eqnarray}
where the last two formulas are called {\it array write axioms}. For
the abstraction, the array write axioms are omitted, and the
abstracted formula $v_1 = v_2$ is processed by the remaining phases of
STP.  As with array reads, the refinement loop iterates only if STP
finds a model of the abstracted formula that is also not a model of
the original formula. Write axioms are added to the abstracted
formula, and the refinement loop iterates with the additional axioms
until a definite result is produced. Although, this technique leads to
improvement in certain cases, the primary problem with it is that the
number of iterations of the refinement loop is sometimes very large.

%% The refinement process is very similar to the one 
%% for array reads. If the SAT solver says that
%% the abstracted formula is unsatisfiable then indeed the original
%% formula is unsatisfiable, and STP says so. Otherwise, the SAT solver
%% can return a satisfying assignment. If this satsifying assignment
%% checks out against the original formula, then STP returns
%% ``satisfiable''. Otherwise it refines by adding those write axioms to
%% the SAT solver that are falsified by the current satisfying
%% assignment. The process is repeated until the correct answer is
%% obtained. This iteration, called the {\it the refinement loop} is
%% gauranteed to terminate since the number of write axioms is finite,
%% and the same satisfying assignment will not be generated repeatedly.

